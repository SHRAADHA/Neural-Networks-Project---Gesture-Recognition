{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6NewGG6Em2G"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, we are going to build a 3D Conv model that will be able to predict the 5 gestures correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4rp2bP1Em2K"
      },
      "source": [
        "By:\n",
        "1. Abhishek Das\n",
        "2. Shradha NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNBnUKxVEm2L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from imageio import imread\n",
        "from skimage.transform import resize\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3tkFNLUEm2M"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTeZ_dnfEm2M"
      },
      "outputs": [],
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ql7Z5e9Em2N"
      },
      "source": [
        "In this block, we will read the folder names for training and validation. We also set the `batch_size` here. Note that we set the batch size in such a way that we are able to use the GPU in full capacity. We kept increasing the batch size until the machine throws an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWvAFQVvEm2N"
      },
      "outputs": [],
      "source": [
        "train_doc = np.random.permutation(open('./Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('./Project_data/val.csv').readlines())\n",
        "batch_size = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqyTZpdlEm2O"
      },
      "source": [
        "## Generator\n",
        "In the generator, we are going to preprocess the images as we have images of 2 different dimensions as well as create a batch of video frames. We have to experiment with `img_idx`, `y`,`z` and normalization such that we get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGC_8ZWvEm2O"
      },
      "outputs": [],
      "source": [
        "def generator(source_path, folder_list, batch_size):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    img_idx = np.round(np.linspace(0,29,16)).astype(int) #create a list of image numbers you want to use for a particular video\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size,len(img_idx),120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    \n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "                    \n",
        "                    image = resize(image,(120,120))\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
        "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
        "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
        "                    \n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "        rem_image = len(folder_list)%batch_size\n",
        "        batch += 1\n",
        "        if(rem_image!=0):\n",
        "            batch_data = np.zeros((rem_image,len(img_idx),120,120,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((rem_image,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(rem_image): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    \n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "                   \n",
        "                    image = resize(image,(120,120))\n",
        "                    batch_data[folder,idx,:,:,0] = (image[:,:,0])/255\n",
        "                    batch_data[folder,idx,:,:,1] = (image[:,:,1])/255\n",
        "                    batch_data[folder,idx,:,:,2] = (image[:,:,2])/255\n",
        "                    \n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI9fNZ99Em2P"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5uoHpF5Em2Q",
        "outputId": "45b20763-49fe-45a1-8c17-764f3f3eb941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 20\n"
          ]
        }
      ],
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = './Project_data/train'\n",
        "val_path = './Project_data/val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 20 # choose the number of epochs\n",
        "print ('# epochs =', num_epochs)\n",
        "num_classes = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTHfPQy1Em2Q"
      },
      "source": [
        "## Model\n",
        "Here we make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. We want to use `TimeDistributed` while building a Conv2D + RNN model. Also that the last layer is the softmax. We design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlUtMt5EEm2R"
      },
      "source": [
        "### Model 1 - CONV3D Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXQNSRejEm2R"
      },
      "outputs": [],
      "source": [
        "import keras as Keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D, AveragePooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "\n",
        "filtersize=(3,3,3)\n",
        "dropout=0.5\n",
        "dense_neurons=256\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(16, filtersize, padding='same',input_shape=(16,120,120,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "        \n",
        "model.add(Conv3D(16, filtersize, padding='same',input_shape=(16,120,120,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "        \n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "model.add(Conv3D(32, filtersize, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "        \n",
        "model.add(Conv3D(32, filtersize, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "        \n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "model.add(Conv3D(64, filtersize, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "        \n",
        "model.add(Conv3D(64, filtersize, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "        \n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "model.add(Conv3D(128, filtersize, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "        \n",
        "model.add(Conv3D(128, filtersize, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "        \n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "        \n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(dense_neurons,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Dense(dense_neurons,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(dropout))\n",
        "\n",
        "model.add(Dense(num_classes,activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3G0mWceEm2R"
      },
      "source": [
        "Now that we have written the model, the next step is to `compile` the model. When we print the `summary` of the model, we'll see the total number of parameters we have to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SPaTyA2bEm2R",
        "outputId": "b68b7358-bd60-4947-f14f-bcebb345d66b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_16 (Conv3D)          (None, 16, 120, 120, 16)  1312      \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 16, 120, 120, 16)  0         \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 16, 120, 120, 16)  64       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_17 (Conv3D)          (None, 16, 120, 120, 16)  6928      \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 16, 120, 120, 16)  0         \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 16, 120, 120, 16)  64       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling3d_8 (MaxPooling  (None, 8, 60, 60, 16)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_18 (Conv3D)          (None, 8, 60, 60, 32)     13856     \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 8, 60, 60, 32)     0         \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 8, 60, 60, 32)    128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_19 (Conv3D)          (None, 8, 60, 60, 32)     27680     \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 8, 60, 60, 32)     0         \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 8, 60, 60, 32)    128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling3d_9 (MaxPooling  (None, 4, 30, 30, 32)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_20 (Conv3D)          (None, 4, 30, 30, 64)     55360     \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 4, 30, 30, 64)     0         \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 4, 30, 30, 64)    256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_21 (Conv3D)          (None, 4, 30, 30, 64)     110656    \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 4, 30, 30, 64)     0         \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 4, 30, 30, 64)    256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling3d_10 (MaxPoolin  (None, 2, 15, 15, 64)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_22 (Conv3D)          (None, 2, 15, 15, 128)    221312    \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 2, 15, 15, 128)    0         \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 2, 15, 15, 128)   512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3d_23 (Conv3D)          (None, 2, 15, 15, 128)    442496    \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 2, 15, 15, 128)    0         \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 2, 15, 15, 128)   512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling3d_11 (MaxPoolin  (None, 1, 7, 7, 128)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               1605888   \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,556,533\n",
            "Trainable params: 2,554,549\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "optimiser = tf.keras.optimizers.Adam(learning_rate=0.0002)#write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J1UEqq1Em2S"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit`and not `fit_generator` as it is deprecated and will be removed in a future version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ejbg8bPEm2S"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEMnji3BEm2S",
        "outputId": "6eaf197a-269e-4098-943f-600e8281a5ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'final_conv3d_model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the REducelronplateau code here\n",
        "\n",
        "callbacks_list = [checkpoint, LR]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArWycR_tEm2S"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgEDGg0QEm2T"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TmO-ChiEm2T"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, we'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDMrJ-ndEm2T",
        "outputId": "13cb9005-7efe-4469-c28a-9f70b208e256"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source path =  ./Project_data/train ; batch size = 20\n",
            "Epoch 1/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 2.1605 - categorical_accuracy: 0.2956 Source path =  ./Project_data/val ; batch size = 20\n",
            "\n",
            "Epoch 1: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00001-2.16045-0.29563-2.27287-0.21000.h5\n",
            "34/34 [==============================] - 738s 22s/step - loss: 2.1605 - categorical_accuracy: 0.2956 - val_loss: 2.2729 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.6117 - categorical_accuracy: 0.4540 \n",
            "Epoch 2: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00002-1.61167-0.45400-3.79728-0.19000.h5\n",
            "34/34 [==============================] - 793s 23s/step - loss: 1.6117 - categorical_accuracy: 0.4540 - val_loss: 3.7973 - val_categorical_accuracy: 0.1900 - lr: 2.0000e-04\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.4859 - categorical_accuracy: 0.5068 \n",
            "Epoch 3: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00003-1.48588-0.50679-3.80125-0.25000.h5\n",
            "\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
            "34/34 [==============================] - 762s 22s/step - loss: 1.4859 - categorical_accuracy: 0.5068 - val_loss: 3.8012 - val_categorical_accuracy: 0.2500 - lr: 2.0000e-04\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.2627 - categorical_accuracy: 0.5581 \n",
            "Epoch 4: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00004-1.26270-0.55807-4.54405-0.21000.h5\n",
            "34/34 [==============================] - 782s 23s/step - loss: 1.2627 - categorical_accuracy: 0.5581 - val_loss: 4.5440 - val_categorical_accuracy: 0.2100 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.1317 - categorical_accuracy: 0.5822 \n",
            "Epoch 5: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00005-1.13168-0.58220-5.54465-0.19000.h5\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "34/34 [==============================] - 790s 23s/step - loss: 1.1317 - categorical_accuracy: 0.5822 - val_loss: 5.5447 - val_categorical_accuracy: 0.1900 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.1016 - categorical_accuracy: 0.6154 \n",
            "Epoch 6: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00006-1.10165-0.61538-5.97531-0.21000.h5\n",
            "34/34 [==============================] - 790s 23s/step - loss: 1.1016 - categorical_accuracy: 0.6154 - val_loss: 5.9753 - val_categorical_accuracy: 0.2100 - lr: 5.0000e-05\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.9387 - categorical_accuracy: 0.6546 \n",
            "Epoch 7: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00007-0.93867-0.65460-6.18201-0.24000.h5\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "34/34 [==============================] - 789s 23s/step - loss: 0.9387 - categorical_accuracy: 0.6546 - val_loss: 6.1820 - val_categorical_accuracy: 0.2400 - lr: 5.0000e-05\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.8916 - categorical_accuracy: 0.6667 \n",
            "Epoch 8: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00008-0.89162-0.66667-6.94369-0.13000.h5\n",
            "34/34 [==============================] - 796s 23s/step - loss: 0.8916 - categorical_accuracy: 0.6667 - val_loss: 6.9437 - val_categorical_accuracy: 0.1300 - lr: 2.5000e-05\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.8671 - categorical_accuracy: 0.6848 \n",
            "Epoch 9: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00009-0.86706-0.68477-5.60489-0.29000.h5\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "34/34 [==============================] - 806s 24s/step - loss: 0.8671 - categorical_accuracy: 0.6848 - val_loss: 5.6049 - val_categorical_accuracy: 0.2900 - lr: 2.5000e-05\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.8677 - categorical_accuracy: 0.6546 \n",
            "Epoch 10: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00010-0.86771-0.65460-6.98173-0.19000.h5\n",
            "34/34 [==============================] - 813s 24s/step - loss: 0.8677 - categorical_accuracy: 0.6546 - val_loss: 6.9817 - val_categorical_accuracy: 0.1900 - lr: 1.2500e-05\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.7971 - categorical_accuracy: 0.6938 \n",
            "Epoch 11: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00011-0.79707-0.69382-5.96181-0.24000.h5\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "34/34 [==============================] - 810s 24s/step - loss: 0.7971 - categorical_accuracy: 0.6938 - val_loss: 5.9618 - val_categorical_accuracy: 0.2400 - lr: 1.2500e-05\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.7631 - categorical_accuracy: 0.7195 \n",
            "Epoch 12: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00012-0.76305-0.71946-5.53784-0.24000.h5\n",
            "34/34 [==============================] - 804s 24s/step - loss: 0.7631 - categorical_accuracy: 0.7195 - val_loss: 5.5378 - val_categorical_accuracy: 0.2400 - lr: 6.2500e-06\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.7925 - categorical_accuracy: 0.7119 \n",
            "Epoch 13: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00013-0.79252-0.71192-4.66692-0.24000.h5\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
            "34/34 [==============================] - 803s 23s/step - loss: 0.7925 - categorical_accuracy: 0.7119 - val_loss: 4.6669 - val_categorical_accuracy: 0.2400 - lr: 6.2500e-06\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.8407 - categorical_accuracy: 0.6878 \n",
            "Epoch 14: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00014-0.84067-0.68778-3.72005-0.25000.h5\n",
            "34/34 [==============================] - 817s 24s/step - loss: 0.8407 - categorical_accuracy: 0.6878 - val_loss: 3.7201 - val_categorical_accuracy: 0.2500 - lr: 3.1250e-06\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.7438 - categorical_accuracy: 0.7104 \n",
            "Epoch 15: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00015-0.74377-0.71041-3.46555-0.28000.h5\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
            "34/34 [==============================] - 786s 23s/step - loss: 0.7438 - categorical_accuracy: 0.7104 - val_loss: 3.4656 - val_categorical_accuracy: 0.2800 - lr: 3.1250e-06\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.7512 - categorical_accuracy: 0.7225 \n",
            "Epoch 16: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00016-0.75117-0.72247-2.57129-0.30000.h5\n",
            "34/34 [==============================] - 780s 23s/step - loss: 0.7512 - categorical_accuracy: 0.7225 - val_loss: 2.5713 - val_categorical_accuracy: 0.3000 - lr: 1.5625e-06\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.7865 - categorical_accuracy: 0.7149 \n",
            "Epoch 17: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00017-0.78649-0.71493-2.01406-0.33000.h5\n",
            "34/34 [==============================] - 789s 23s/step - loss: 0.7865 - categorical_accuracy: 0.7149 - val_loss: 2.0141 - val_categorical_accuracy: 0.3300 - lr: 1.5625e-06\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.7758 - categorical_accuracy: 0.6848 \n",
            "Epoch 18: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00018-0.77577-0.68477-1.63348-0.47000.h5\n",
            "34/34 [==============================] - 718s 21s/step - loss: 0.7758 - categorical_accuracy: 0.6848 - val_loss: 1.6335 - val_categorical_accuracy: 0.4700 - lr: 1.5625e-06\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.7330 - categorical_accuracy: 0.7164 \n",
            "Epoch 19: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00019-0.73298-0.71644-1.44688-0.49000.h5\n",
            "34/34 [==============================] - 665s 19s/step - loss: 0.7330 - categorical_accuracy: 0.7164 - val_loss: 1.4469 - val_categorical_accuracy: 0.4900 - lr: 1.5625e-06\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34/34 [==============================] - ETA: 0s - loss: 0.7512 - categorical_accuracy: 0.7149 \n",
            "Epoch 20: saving model to final_conv3d_model_2022-05-0812_35_28.069255/model-00020-0.75115-0.71493-1.11872-0.62000.h5\n",
            "34/34 [==============================] - 635s 19s/step - loss: 0.7512 - categorical_accuracy: 0.7149 - val_loss: 1.1187 - val_categorical_accuracy: 0.6200 - lr: 1.5625e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe471e63af0>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN5vgyilEm2T"
      },
      "source": [
        "### Model 2 - CONV2D + LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uJOiKVrEm2T"
      },
      "outputs": [],
      "source": [
        "model_2 = Sequential()\n",
        "\n",
        "model_2.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=(16, 120, 120, 3)))\n",
        "model_2.add(TimeDistributed(BatchNormalization()))\n",
        "model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "model_2.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
        "model_2.add(TimeDistributed(BatchNormalization()))\n",
        "model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "model_2.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
        "model_2.add(TimeDistributed(BatchNormalization()))\n",
        "model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "model_2.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
        "model_2.add(TimeDistributed(BatchNormalization()))\n",
        "model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "model_2.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n",
        "model_2.add(TimeDistributed(BatchNormalization()))\n",
        "model_2.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "model_2.add(TimeDistributed(Flatten()))\n",
        "model_2.add(LSTM(256))\n",
        "model_2.add(Dropout(0.25))\n",
        "        \n",
        "model_2.add(Dense(128,activation='relu'))\n",
        "model_2.add(Dropout(0.25))\n",
        "model_2.add(Dense(num_classes, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu0DloK1Em2U",
        "outputId": "c58ab93e-13f1-4f85-e8ec-530f845ff742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed_48 (TimeDi  (None, 16, 120, 120, 16)  448      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_49 (TimeDi  (None, 16, 120, 120, 16)  64       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_50 (TimeDi  (None, 16, 60, 60, 16)   0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_51 (TimeDi  (None, 16, 60, 60, 32)   4640      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_52 (TimeDi  (None, 16, 60, 60, 32)   128       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_53 (TimeDi  (None, 16, 30, 30, 32)   0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_54 (TimeDi  (None, 16, 30, 30, 64)   18496     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_55 (TimeDi  (None, 16, 30, 30, 64)   256       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_56 (TimeDi  (None, 16, 15, 15, 64)   0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_57 (TimeDi  (None, 16, 15, 15, 128)  73856     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_58 (TimeDi  (None, 16, 15, 15, 128)  512       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_59 (TimeDi  (None, 16, 7, 7, 128)    0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_60 (TimeDi  (None, 16, 7, 7, 256)    295168    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_61 (TimeDi  (None, 16, 7, 7, 256)    1024      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_62 (TimeDi  (None, 16, 3, 3, 256)    0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_63 (TimeDi  (None, 16, 2304)         0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 256)               2622464   \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,050,597\n",
            "Trainable params: 3,049,605\n",
            "Non-trainable params: 992\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "optimiser = tf.keras.optimizers.Adam(learning_rate=0.0002) #write your optimizer\n",
        "model_2.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model_2.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z1o_XNmEm2U"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit`and not `fit_generator` as it is deprecated and will be removed in a future version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiedAsZJEm2U"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYBOLn_wEm2U",
        "outputId": "4137474d-1adb-411c-87e8-fa4fde44eb32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'final_conv2d+lstm_model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
        "        \n",
        "callbacks_list = [checkpoint, LR]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QVuwrUBEm2U"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDm5AK7BEm2U"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, we'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNxTOgQtEm2U",
        "outputId": "358ed8bc-2894-41c5-ceb2-ac543f2702b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source path =  ./Project_data/train ; batch size = 20\n",
            "Epoch 1/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 1.3910 - categorical_accuracy: 0.3831Source path =  ./Project_data/val ; batch size = 20\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 1.68429, saving model to final_conv2d+lstm_model_2022-05-0812_35_28.069255/model-00001-1.39097-0.38311-1.68429-0.21000.h5\n",
            "34/34 [==============================] - 126s 4s/step - loss: 1.3910 - categorical_accuracy: 0.3831 - val_loss: 1.6843 - val_categorical_accuracy: 0.2100 - lr: 2.0000e-04\n",
            "Epoch 2/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.9981 - categorical_accuracy: 0.6169\n",
            "Epoch 2: val_loss did not improve from 1.68429\n",
            "34/34 [==============================] - 123s 4s/step - loss: 0.9981 - categorical_accuracy: 0.6169 - val_loss: 1.7362 - val_categorical_accuracy: 0.2400 - lr: 2.0000e-04\n",
            "Epoch 3/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.8439 - categorical_accuracy: 0.6878\n",
            "Epoch 3: val_loss did not improve from 1.68429\n",
            "\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
            "34/34 [==============================] - 122s 4s/step - loss: 0.8439 - categorical_accuracy: 0.6878 - val_loss: 1.8629 - val_categorical_accuracy: 0.2000 - lr: 2.0000e-04\n",
            "Epoch 4/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.6665 - categorical_accuracy: 0.7511\n",
            "Epoch 4: val_loss did not improve from 1.68429\n",
            "34/34 [==============================] - 123s 4s/step - loss: 0.6665 - categorical_accuracy: 0.7511 - val_loss: 2.0357 - val_categorical_accuracy: 0.1900 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.4986 - categorical_accuracy: 0.8371\n",
            "Epoch 5: val_loss did not improve from 1.68429\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "34/34 [==============================] - 129s 4s/step - loss: 0.4986 - categorical_accuracy: 0.8371 - val_loss: 2.1417 - val_categorical_accuracy: 0.2800 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.3880 - categorical_accuracy: 0.8899\n",
            "Epoch 6: val_loss did not improve from 1.68429\n",
            "34/34 [==============================] - 136s 4s/step - loss: 0.3880 - categorical_accuracy: 0.8899 - val_loss: 2.4807 - val_categorical_accuracy: 0.2100 - lr: 5.0000e-05\n",
            "Epoch 7/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.3142 - categorical_accuracy: 0.9276\n",
            "Epoch 7: val_loss did not improve from 1.68429\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "34/34 [==============================] - 134s 4s/step - loss: 0.3142 - categorical_accuracy: 0.9276 - val_loss: 2.4866 - val_categorical_accuracy: 0.2000 - lr: 5.0000e-05\n",
            "Epoch 8/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.2916 - categorical_accuracy: 0.9276\n",
            "Epoch 8: val_loss did not improve from 1.68429\n",
            "34/34 [==============================] - 141s 4s/step - loss: 0.2916 - categorical_accuracy: 0.9276 - val_loss: 2.5507 - val_categorical_accuracy: 0.2200 - lr: 2.5000e-05\n",
            "Epoch 9/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.2589 - categorical_accuracy: 0.9336\n",
            "Epoch 9: val_loss did not improve from 1.68429\n",
            "\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "34/34 [==============================] - 137s 4s/step - loss: 0.2589 - categorical_accuracy: 0.9336 - val_loss: 2.6796 - val_categorical_accuracy: 0.1600 - lr: 2.5000e-05\n",
            "Epoch 10/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.2316 - categorical_accuracy: 0.9472\n",
            "Epoch 10: val_loss did not improve from 1.68429\n",
            "34/34 [==============================] - 133s 4s/step - loss: 0.2316 - categorical_accuracy: 0.9472 - val_loss: 2.5189 - val_categorical_accuracy: 0.2700 - lr: 1.2500e-05\n",
            "Epoch 11/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.2109 - categorical_accuracy: 0.9563\n",
            "Epoch 11: val_loss did not improve from 1.68429\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "34/34 [==============================] - 135s 4s/step - loss: 0.2109 - categorical_accuracy: 0.9563 - val_loss: 2.6465 - val_categorical_accuracy: 0.3000 - lr: 1.2500e-05\n",
            "Epoch 12/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.2060 - categorical_accuracy: 0.9698\n",
            "Epoch 12: val_loss did not improve from 1.68429\n",
            "34/34 [==============================] - 132s 4s/step - loss: 0.2060 - categorical_accuracy: 0.9698 - val_loss: 2.6185 - val_categorical_accuracy: 0.3600 - lr: 6.2500e-06\n",
            "Epoch 13/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.1845 - categorical_accuracy: 0.9638\n",
            "Epoch 13: val_loss did not improve from 1.68429\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n",
            "34/34 [==============================] - 130s 4s/step - loss: 0.1845 - categorical_accuracy: 0.9638 - val_loss: 2.3374 - val_categorical_accuracy: 0.4000 - lr: 6.2500e-06\n",
            "Epoch 14/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.1849 - categorical_accuracy: 0.9729\n",
            "Epoch 14: val_loss did not improve from 1.68429\n",
            "34/34 [==============================] - 132s 4s/step - loss: 0.1849 - categorical_accuracy: 0.9729 - val_loss: 2.0443 - val_categorical_accuracy: 0.4000 - lr: 3.1250e-06\n",
            "Epoch 15/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.2005 - categorical_accuracy: 0.9683\n",
            "Epoch 15: val_loss did not improve from 1.68429\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n",
            "34/34 [==============================] - 136s 4s/step - loss: 0.2005 - categorical_accuracy: 0.9683 - val_loss: 1.9323 - val_categorical_accuracy: 0.4000 - lr: 3.1250e-06\n",
            "Epoch 16/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.1837 - categorical_accuracy: 0.9744\n",
            "Epoch 16: val_loss improved from 1.68429 to 1.48110, saving model to final_conv2d+lstm_model_2022-05-0812_35_28.069255/model-00016-0.18368-0.97436-1.48110-0.50000.h5\n",
            "34/34 [==============================] - 135s 4s/step - loss: 0.1837 - categorical_accuracy: 0.9744 - val_loss: 1.4811 - val_categorical_accuracy: 0.5000 - lr: 1.5625e-06\n",
            "Epoch 17/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.1864 - categorical_accuracy: 0.9623\n",
            "Epoch 17: val_loss improved from 1.48110 to 1.19222, saving model to final_conv2d+lstm_model_2022-05-0812_35_28.069255/model-00017-0.18643-0.96229-1.19222-0.53000.h5\n",
            "34/34 [==============================] - 136s 4s/step - loss: 0.1864 - categorical_accuracy: 0.9623 - val_loss: 1.1922 - val_categorical_accuracy: 0.5300 - lr: 1.5625e-06\n",
            "Epoch 18/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.1822 - categorical_accuracy: 0.9759\n",
            "Epoch 18: val_loss improved from 1.19222 to 0.98761, saving model to final_conv2d+lstm_model_2022-05-0812_35_28.069255/model-00018-0.18225-0.97587-0.98761-0.61000.h5\n",
            "34/34 [==============================] - 137s 4s/step - loss: 0.1822 - categorical_accuracy: 0.9759 - val_loss: 0.9876 - val_categorical_accuracy: 0.6100 - lr: 1.5625e-06\n",
            "Epoch 19/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.1912 - categorical_accuracy: 0.9593\n",
            "Epoch 19: val_loss improved from 0.98761 to 0.88214, saving model to final_conv2d+lstm_model_2022-05-0812_35_28.069255/model-00019-0.19117-0.95928-0.88214-0.57000.h5\n",
            "34/34 [==============================] - 138s 4s/step - loss: 0.1912 - categorical_accuracy: 0.9593 - val_loss: 0.8821 - val_categorical_accuracy: 0.5700 - lr: 1.5625e-06\n",
            "Epoch 20/20\n",
            "34/34 [==============================] - ETA: 0s - loss: 0.1890 - categorical_accuracy: 0.9713\n",
            "Epoch 20: val_loss improved from 0.88214 to 0.69811, saving model to final_conv2d+lstm_model_2022-05-0812_35_28.069255/model-00020-0.18897-0.97134-0.69811-0.70000.h5\n",
            "34/34 [==============================] - 140s 4s/step - loss: 0.1890 - categorical_accuracy: 0.9713 - val_loss: 0.6981 - val_categorical_accuracy: 0.7000 - lr: 1.5625e-06\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe4196b7190>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxxrpYgPEm2U"
      },
      "source": [
        "## ---- END ----"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Gesture_Recognition_Case_Study.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}